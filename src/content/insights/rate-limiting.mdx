---
title: "Handling Rate Limiting and Throttling"
description: "Protect your API and provide graceful degradation under load"
date: "2025-03-24"
---

Rate limiting protects your backend, improves fairness, and prevents abuse. Learn token bucket algorithms, sliding window techniques, and graceful degradation strategies.

## Rate Limiting Strategies

### 1. Token Bucket Algorithm

```typescript
interface TokenBucket {
  tokens: number;
  refillRate: number; // tokens per second
  maxTokens: number;
  lastRefill: number;
}

function consumeTokens(bucket: TokenBucket, count: number = 1): boolean {
  const now = Date.now() / 1000;
  const timePassed = now - bucket.lastRefill;

  // Refill tokens
  bucket.tokens = Math.min(
    bucket.maxTokens,
    bucket.tokens + timePassed * bucket.refillRate
  );
  bucket.lastRefill = now;

  if (bucket.tokens >= count) {
    bucket.tokens -= count;
    return true;
  }
  return false;
}
```

### 2. Sliding Window (Redis)

```typescript
import { redis } from "@/lib/redis";

async function rateLimit(userId: string, limit: number, window: number) {
  const key = `rate_limit:${userId}`;
  const now = Date.now();
  const windowStart = now - window * 1000;

  // Remove old requests outside window
  await redis.zremrangebyscore(key, 0, windowStart);

  // Count requests in window
  const count = await redis.zcard(key);

  if (count >= limit) {
    return { allowed: false, remaining: 0 };
  }

  // Add current request
  await redis.zadd(key, now, `${now}:${Math.random()}`);
  await redis.expire(key, window);

  return { allowed: true, remaining: limit - count - 1 };
}
```

## Graceful Degradation

When limits hit, don't return errors—degrade service:

```typescript
export async function fetchUserData(userId: string) {
  const limit = await checkRateLimit(userId);

  if (limit.allowed) {
    // Fresh data from DB
    return await db.users.findById(userId);
  } else if (limit.remaining === 0) {
    // Cache hit (stale data OK)
    return await redis.get(`user:${userId}`);
  } else {
    // Queue for batch processing
    await queue.add("fetch_user", { userId });
    return await cache.getOrDefault(userId, {
      /* default */
    });
  }
}
```

## Response Headers

```typescript
// Tell clients what to expect
res.headers.set("X-RateLimit-Limit", "1000");
res.headers.set("X-RateLimit-Remaining", "987");
res.headers.set("X-RateLimit-Reset", "1609459200");

// On limit exceeded
res.headers.set("Retry-After", "60");
return new Response("Rate limit exceeded", { status: 429 });
```

## Checklist

- [x] Implement per-user rate limits (prevent abuse).
- [x] Add per-IP limits (DDoS mitigation).
- [x] Implement quota per API key or plan tier.
- [x] Return clear `429 Too Many Requests` with `Retry-After`.
- [x] Provide remaining quota in response headers.
- [x] Cache data gracefully when limits hit.
- [ ] Build dashboard to monitor limit violations.
- [ ] Implement rate limit exemptions for trusted clients.

## Results

- API stability: 99.9% → 99.95% uptime.
- Abuse prevention: malicious traffic blocked automatically.
- Fair resource distribution: no single user dominating.
- Customer satisfaction: SLAs met consistently.

## Edge Cases

- Burst traffic: token bucket allows short bursts within limits.
- Distributed systems: use centralized Redis for global limits.
- Clock skew: validate timestamps; tolerate small drifts.

## Next

- Implement adaptive rate limiting (tighten under high load).
- Build priority queues for VIP customers.
