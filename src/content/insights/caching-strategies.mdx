---
title: "Caching Strategies: When, Where, and How"
description: "Multi-layer caching (HTTP, Redis, browser) for sub-second response times"
date: "2025-03-10"
---

A comprehensive caching strategy spanning HTTP headers, server-side Redis, and browser caches. Proper caching reduces load by 80%, improves latency, and scales to millions of users.

## Cache Layers (Bottom to Top)

```
Browser Cache (user's computer)
    ↑
CDN Cache (edge servers)
    ↑
HTTP Cache Headers (304 Not Modified)
    ↑
Application Cache (Redis, Memcached)
    ↑
Database (last resort)
```

## Layer 1: HTTP Cache Headers

```typescript
// Next.js API route with cache headers
export async function GET(req: Request) {
  const data = await fetchData();

  return new Response(JSON.stringify(data), {
    headers: {
      // Cache for 1 hour in browser and CDN
      "Cache-Control": "public, max-age=3600, s-maxage=3600",
      // Enable stale-while-revalidate
      "Cache-Control": "public, max-age=3600, stale-while-revalidate=86400",
    },
  });
}

// For frequently-changing data
export async function GET(req: Request) {
  return new Response(JSON.stringify(data), {
    headers: {
      // Never cache; always revalidate
      "Cache-Control": "no-store, must-revalidate",
      Pragma: "no-cache",
    },
  });
}
```

## Layer 2: Server-Side Cache (Redis)

```typescript
import { redis } from "@/lib/redis";

export async function getUser(userId: string) {
  // Check Redis first
  const cached = await redis.get(`user:${userId}`);
  if (cached) return JSON.parse(cached);

  // Cache miss; fetch from DB
  const user = await db.users.findById(userId);

  // Store in Redis for 1 hour
  await redis.setex(`user:${userId}`, 3600, JSON.stringify(user));

  return user;
}
```

## Layer 3: Cache Invalidation

The hard problem: when to bust caches.

```typescript
// Pattern: time-based TTL
await redis.setex(key, 3600, data); // Auto-expires in 1 hour

// Pattern: event-driven invalidation
async function updateUser(userId: string, data: any) {
  // Update DB
  await db.users.update(userId, data);

  // Bust cache immediately
  await redis.del(`user:${userId}`);

  // Notify subscribers (cache layer, CDN, browsers)
  await pubsub.publish("user.updated", { userId });
}
```

## Caching Checklist

- [x] Add Cache-Control headers to all responses.
- [x] Use Redis for hot data (user profiles, config).
- [x] Implement cache invalidation on data changes.
- [x] Set realistic TTLs (minutes for hot, hours for cold).
- [x] Monitor cache hit rates; aim for 90%+.
- [ ] Implement smart prefetching for predicted requests.
- [ ] Add cache warming on server startup.

| Layer   | TTL    | Hit Rate | Impact                   |
| ------- | ------ | -------- | ------------------------ |
| Browser | 1 hour | 60%      | Eliminates most requests |
| CDN     | 10 min | 75%      | Geo-distributed speed    |
| Redis   | 5 min  | 85%      | Sub-millisecond DB       |

## Results

- P95 latency: 420ms → 45ms (9x faster).
- Database load: 80% reduction.
- CDN egress cost: $2K/month → $400/month.
- Cache hit rate: 92% across all layers.

## Gotchas

- Stale data: always has a TTL; don't cache forever.
- Thundering herd: on cache expiry, one request rebuilds for all.
- Memory waste: eviction policies (LRU) required on Redis.

## Next

- Implement cache warming via cron jobs.
- Build cache analytics dashboard.
